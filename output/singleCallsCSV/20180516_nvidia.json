[{"title":"NVIDIA Corporation (NVDA) Q1 2019 Earnings Conference Call Transcript","callText":"\n                                            \n                                                \nImage source: The Motley Fool.\n\nNVIDIA Corporation (NASDAQ:NVDA)Q1 2019 Earnings Conference CallMay 10, 2018, 5:00 p.m. ET\nContents:\n\nPrepared Remarks\nQuestions and Answers\nCall Participants\n\nPrepared Remarks:\nOperator\nGood afternoon. My name is Kelsey and I am your conference operator for today. Welcome to NVIDIA's Financial Results Conference Call. All lines have been placed on mute. After the speakers' remarks, there will be question-and-answer period. At this time, if you would like to ask a question, please press * then the number 1 on your telephone keypad. To withdraw your question, press the # key. Thank you. I'll now turn the call over to Simona Jankowski, Vice President of Investor Relations, to begin your conference.\nSimona Jankowski -- Vice President, Investor Relations\nThank you. Good afternoon everyone and welcome to NVIDIA's conference call for the first quarter of fiscal 2019. With me on the call today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer.\nI'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. It is also being recorded. You can hear a replay by telephone until May 16, 2018. The webcast will be available for replay up the conference call to discuss our financial results for the second quarter of fiscal 2019.\n\nThe content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent.\nDuring this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Form 10, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 10, 2018, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements.\nDuring this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO Commentary, which is posted on our website. With that, let me turn the call over to Colette.\nColette Kress -- Executive Vice President and Chief Financial Officer\nThanks, Simona. We had an excellent quarter with growth across all our performs, led by gaming and data center. Q1 revenue reached a record $3.21 billion, up 66% year-on-year, up 10% sequentially, and above our outlook of $2.9 billion. Once again, all measures of profitability set records. With GAAP gross margins at 64.5%, operating margin at 40.4%, and net income at $1.24 billion. From a reporting segment perspective, Q1 GPU revenue grew 77% from last year to $2.77 billion. Tegra Processor revenue rose 33% to $442 million.\nLet's start with our gaming business. Revenue was $1.72 billion, up 68% year-on-year, and down 1% sequentially. Demand was strong and broad-based across regions and products. The gaming market remains robust and the popular battle royale genre is attracting a new wave of gamers to the GeForce platform. We also continue to see demand from upgrades, with about 35% of our installed base currently on our Pascal architecture. The launch of popular titles like Far Cry 5, and Final Fantasy XV continue to drive excitement in the quarter.\nGamers are increasingly engaging in social gameplay and gaming is rapidly becoming a spectator sport, while the production value of games continues to increase. This dynamic is fueling a virtuous cycle that expands the universe of gamers and drives a mix shift to higher-end GPUs. At the recent Game Developers Conference, we announced our real-time, ray tracing technology -- NVIDIA RTX. Ray tracing is movie-quality rendering technique that delivers lifelike lighting, reflections and shadows. It has long been considered the holy grail of graphics and we've been working on it for over 10 years. We look forward to seeing amazing cinematic games that take advantage of this technology come to the market later this year, with the pipeline building into next year and beyond.\nWe expect RTX, as well as other new technologies like 4K and virtual reality, to continue driving gamers' requirements for higher GPU performance. While supply was tight earlier in the quarter, the situation is now easing. As a result, we are pleased to see that channel prices for our GPUs are beginning to normalize, allowing gamers who had been priced out of the market last quarter to get their hands on a new GeForce GTX at a reasonable price.\nCryptocurrency demand was, again, stronger than expected, but we were able to fulfill most of it with crypto-specific GPUs, which are included in our OEM business at $289 million. As a result, we could protect the vast majority of our limited gaming GPU supply for use by gamers. Looking into Q2, we expect crypto-specific revenue to be about one-third of its Q1 level. Gaming notebooks also grew well, driven by an increasing number of thin and light notebooks based on our Max-Q design. Nintendo Switch contributed strongly to year-on-year growth, reflecting that platform's continued success.\nMoving to datacenter, we had another phenomenal quarter, with revenue of $701 million, up 71% year-on-year, up 16% sequentially. Demand was strong in all market segments and customers increasingly embraced our GPUs and CUDA platform for high performance computing and AI.\nAdoption of our Volta architecture remains strong across a wide range of verticals and customers. In the public cloud segment, Microsoft Azure announced general availability of Tesla V100 inferences, joining Amazon, IBM, and Oracle, and Google Cloud announced that the V100 is now publicly available in Beta. Many other hyperscale and consumer internet companies also continued the ramp of Volta, which delivers 5 times the deep learning of its predecessor, Pascal. Volta has been chosen by every major cloud provider and server maker, reinforcing our leadership in AI deep learning.\nIn high-performance computing, strength from the broad enterprise vertical more than offset the ramp-down of major supercomputing projects, such as the U.S. Department of Energy's Summit system. We see a strong pipeline across a number of vertical industries from manufacturing to oil and gas, which will help it sustain the trajectory of high-performance computing next quarter and beyond.\nTraction is also increasing in AI inference. Inference GPU shipments to cloud service providers more that doubled from last quarter, and our pipeline is growing into next quarter. We dramatically increased our inference capabilities with the announcement of the TensorRT 4 AI inference accelerator software at our recent GPU technology conference in San Jose. TensorRT 4 accelerates deep learning inference up to 190 times faster than CPUs for common applications such as computer vision, neural machine translation, automatic speech recognition, speech synthesis, and recognition systems. It also dramatically expands the use cases prepared with the prior version. With TensorRT 4, NVIDIA's market reach has expanded to approximately 30 million hyperscale servers worldwide.\nAt DTC, we also announced other major advancements in our deep learning platform. We doubled the memory of [inaudible] to 32 gig [inaudible], which is a key [inaudible] for customers building large neural networks from larger datasets. And we announced a new GPU [inaudible] called NVIDIA [inaudible]. [Inaudible] Tesla V100 GPUs at a speed of 2.4 terabytes per second for 5 times faster than the best PCIU switch. We also announced our DGX-2 system, which leverages these new technologies and is updated, fully optimized software stack to deliver a 10X performance boost beyond last year's DGX. DGX-2 is the first single server capable of delivering [inaudible] of computational power.\nWe're receiving strong interest from both hyperscale and [inaudible] customers and we look forward to bringing this technology to cloud customers later this year. At our investor day in March, we updated our forecast for the data center and the rest of the market. We see the data center opportunity as very large, fueled by growing demand for accelerated computing and applications ranging from AI to high-performance computing across multiple market segments and vertical industries.\nWe estimate the TAM at $50 billion by 2023, which extends our previous forecast of $30 billion by 2020.We see strong momentum in the adoption of our accelerated computing platform and the expansion of our development ecosystem to serve this rapidly growing market. About 8,500 attendees registered for GTC, up 18% from last year. CUDA downloads have continued to grow, setting a fresh record in the quarter, and our total number of developers is well over 850,000, up 72% from last year.\nMoving to pro visualization, revenue grew to $251 million, up 22% from a year ago, and accelerating from last quarter, driven by demand for real-time rendering, as well as emerging applications like AI and VR. Strength extended across several key industries, including public sector, healthcare, and retail. Key winds in the quarter included Columbia University using high-end QUADRO GPUs for AI and Siemens using them for CT and ultrasound solutions.\nAt GTC, we announced the QUADRO GV100 GPU with NVIDIA RTX technology, capable of delivering real-time ray tracing to the more than 25 million artists and designers throughout the world. RTX makes computational intensive ray tracing possible in real-time when running professional design and content creation applications. This allows media and entertainment professionals to see and interact with their creations with correct light and shadows and do complex renders up to 10 times faster than a CPU alone.\nAnd the NVIDIA OptiX AI-Accelerated Denoiser built into RTX delivers almost 100 times the preference of CPUs for real-time, noise-free rendering. This enables customers to replace racks of servers in traditional render farms with GPU servers at one-fifth the cost, one-seventh the space, and one-seventh the power. Lastly, automotive. Revenue grew 4% year-on-year to a record $145 million. This reflects ongoing transition from our infotainment business to our growing autonomous vehicle development and production opportunities around the globe.\nAt DTC and investor day, we made key product announcements on the advancement of autonomous vehicles and established a total addressable market opportunity of $60 billion by 2035. We believe that every vehicle will be autonomous one day. By 2035, this will encompass 100 million autonomous passenger vehicles and 10 million robo-taxis. We also introduced NVIDIA DRIVE Constellation, a platform that will help [inaudible], [inaudible] makers, [inaudible] suppliers, and other developing autonomous vehicle test and validate their systems in a virtual world across a wide range of scenarios before deploying on the road.\nEach year, 10 trillion miles are driven around the world. Even if test cars can eventually cover millions of miles, that's an insignificant fraction of all the scenarios that require testing to create a safe and reliable, autonomous vehicle. DRIVE Constellation addresses this challenge by [inaudible] cars to safety drive billions of miles in virtual reality. The platform has two different servers. The first is loaded with GPUs and simulates the environment that the car is driving in, as in a hyper-real video game.\nThe second contains the NVIDIA DRIVE Pegasus autonomous vehicle computer which possesses the simulated data as if it were coming from the sensors of a car driving on the road. Real-time driving commands from the DRIVE Pegasus are fed back to the simulation for true hardware-in-the-loop verification. Constellation will enable autonomous vehicle industry for safety test and validate their self-driving systems in ways that are not practical or possible with on-road testing.\nWe also extended our product road map to include [inaudible], our next generation DRIVE autonomous vehicle computer. We have created a scalable AI car platform that spans the entire range of autonomous driving from traffic jams, pilots, to Level 5 robo-taxis. More than 370 companies and research institutions are now using NVIDIA's automotive platform. With this growing momentum, we remain excited about the intermediate and long-term opportunities for our autonomous driving business.\nNow, moving to the rest of the P&L. Q1 GAAP gross margins was 64.5% and non-GAAP was 64.7%, records that reflect continued growth in our value-added platforms. GAAP operating expenses were $773 million, non-GAAP operating expenses was $648 million, up 25% year-on-year. We continue to invest in key platforms driving our long-term growth, including gaming, AI, and automotive. GAAP net income was a record $1.24 billion and EPS was $1.98, up 45% and [inaudible]%, respectively from a year earlier. [Inaudible] tax rate of 5% compared to our guidance of 12%. Non-GAAP net income was $1.29 billion and EPS was $2.05, both up 141% from a year ago, reflecting the revenue strength, as well as gross margins and operating margin expansion and slightly lower tax.\nOur quarterly cash flow from operations reached record levels at $1.45 billion. Capital expenditures were $118 million. With that, let me turn to the outlook for the second quarter of Fiscal 2019. We expect revenue to be $3.1 billion, +/- 2%. GAAP and non-GAAP gross margins are expected to be 63.6% and 63.5% respectively, +/- 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $810 million and $685 million respectively. GAAP and non-GAAP OI&E are both expected to be income of approximately $15 million. GAAP and non-GAAP tax rates are both expected to be 11%, +/- 1%, excluding discrete items. Capital expenditures are expected to be approximately $130 million to $150 million.\nFurther financial details are included in the CFO commentary and other information available on our IR website. In closing, I'd like to highlight a few upcoming events for the financial community. We will be presenting at the J.P. Morgan Technology Conference next week on May 15th, and at the Bank of America Global Technology Conference on June 5th. We will also hold our annual meeting of stockholders online on May 16th. We will now open the call for questions. Simona and I are here in Santa Clara, and Jensen is dialing in from the road. Operator, would you please poll for questions? Thank you.\nQuestions and Answers:\nOperator\nOkay. At this time, if you would like to ask a question, please press * then the number 1 on your telephone keypad. Again, that was *1 for questions. Your first question is from Stacy Rasgon with Bernstein Research.\nStacy Rasgon -- Bernstein Research -- Analyst\nHi, guys. Thanks for taking my questions. First, I had a question on gaming seasonality. It's usually down pretty decently in Q1. It was obviously flat this time, as you were trying to fill up the channel. Now that's done, I was just wondering what the supply dynamics, as well as any thoughts on crypto might mean for the seasonality into Q2 versus what would be typical where it would usually be up pretty decently? How are you looking at -- and this is a question for Colette.\nColette Kress -- Executive Vice President and Chief Financial Officer\nJensen, why don't you start on the question for Stacy and I'll follow-up afterwards after you state.\nJensen Huang -- President and Chief Executive Officer\nOkay. Hi, Stacy. So, let's see. Q1. As you probably know, Fortnite and PUBG are global phenomena. The success of Fortnite and PUBG are just beyond comprehension, really. Those two games, a combination of Hunger Games and Survivor, you know, has just captures the imaginations of gamers all over the world. We saw the uptick and we saw the demand in our GPUs from all over the world. Surely, there was scarcity, as you know. Crypto miners bought a lot of our GPUs during the quarter and it drove prices up. I think that a lot of the gamers weren't able to buy into the new GeForces as a result.\nWe're starting to see the prices come down. We monitor spot pricing every single day around the world. The prices are starting to normalize. It's still higher than where they should be. So, obviously, the demand is still quite strong out there. But my sense is that there's a fair amount of pent-up demand still. Fortnite is still growing in popularity. PUBG is doing great. Then we've got some amazing titles coming out. My sense is that the overall gaming market is super healthy.\nOur job is to make sure that we work as hard as we can to get supply out into the marketplace and hopefully by doing that, the pricing will normalize and the gamers can buy into their favorite graphics card at a price that we hope they can get it at. The simple answer to your question is Fortnite and PUBG and the demand is really great. They did a great job.\nOperator\nYour next question is from Joe Moore with Morgan Stanley.\nJoe Moore-Morgan Stanley -- Analyst\nColette had talked about the inference doubling in sales quarter-over-quarter with cloud. Can you just talk about where you're seeing the early applications for inference? Is that sort of as a service business or are you looking at internal cloudware and just any color you can give us on where you guys are sitting in the inference space? Thank you.\nJensen Huang -- President and Chief Executive Officer\nSure. Hi, Joe. As you know, there are 30 million servers around the world. They were put in place during the time when the world didn't have deep learning. Now with deep learning and now with machine learning approaches, the accuracy of prediction, the accuracy of recommendation has jumped so much, that just about every internet service provider in the world that has a lot of different customers and consumers are jumping onto this new software approach.\nIn order to take this neural network -- and the software that's written by deep learning, these frameworks, are massive software. The way to think about these deep neural nets is it has millions and millions and millions of parameters in it and these networks are getting larger every year and they're enormous and complex. The output of these neural nets have to be optimized for the computing platform that it targets. How you would optimize the neural network for a CPU or a GPU is very, very different.\nHow you optimize for different neural networks, whether it's image recognition, speech recognition, natural language translation, recommendation systems, all of these networks have different architectures and the optimizing compiler that's necessary to make the neural network run smoothly and fast is incredibly complex. And so that's why we created TensorRT. That's what TensorRT is. TensorRT is an optimizing graph neural network compiler and it optimizes for each one of our platforms. Each one of our platforms has very different architectures.\nFor example, we reinvented the GPU and it's called a Tensor Core GPU and the first of its kind is called Volta. TensorRT 4.0 now supports, in addition to image recognition, all of the different types of neural network models. The answer to your question is internal consumption. Internal consumption is going to be the first users. Video recognition, detecting for inappropriate video, for example, all over the world, making recommendations from the videos that you search or the images that you're uploading. All of these types of applications are going to require an enormous amount of computation.\nOperator\nThe next question is from Vivek Arya with Bank of America.\nVivek Arya -- Bank of America -- Analyst\nThank you for taking my question and congratulations on the strong growth and consistent execution. Jensen, I have two questions about the data center. One from a growth and the second from a competition perspective. So, from the growth side, you guys are doing about say $3 billion or so annualized, but you have outlined a market that could be $50 billion. What needs to happen for the next inflection? Is it something in the market that needs to change? Is it something in the product set that needs to? How do you go and address that $50 billion market? Because you're only a few percent penetrated today in that large market. So, what needs to change for the next inflection point?\nThen on the competition side, as you are looking at that big market, how should we think about competition that is coming from some of your cloud customers, like Google announcing a TPU 3 or perhaps others looking at other competing technologies? Any color on both sort of how you look at growth and competition would be very helpful. Thank you.\nJensen Huang -- President and Chief Executive Officer\nThanks, Vivek. First of all, at its core, this is something we all know now. That CPU scaling has really slowed. If you think about the several hundred billion dollars' worth of computer equipment that's been installed in the cloud, in data centers all over the world, and as these applications for machine learning and high-performance computing approaches come along, the world needs a solution. CPU scaling has slowed. So, here is the approach that we pioneered a decade and a half ago called GPU computing, and we've been determined to continue to advance it during this time because we saw this day coming and we really believed that it was going to end. You can't definitely physics.\nWe find ourselves in a great position today. As Colette already mentioned, we have something close to a million developers on this platform now. It is incredibly fast, speeding up CPUs by 10, 20, 50 100 times, 200 times sometimes, depending on the algorithm. It's everywhere. The software ecosystem is just super-rich. As Colette mentioned, there's already almost a million developers around the world, that's grown 70% year-over-year.\nI think at the core, it's about the fact that the world needs a computing approach going forward. With respect to our ability to address the TAM, there are three major segments -- there's more than that -- but there's three major segments. One is, of course, training for deep learning. The other is inferencing. TRT 4 is intended to do just that, to expand our ability to address all of the different types of algorithms, machine learning algorithms that are running in the data centers. The third is high-performance computing. That's molecular dynamics to medical imaging, to earth sciences, to energy sciences. The type of algorithms that are being run in super computers all over the world is expanding.\nWe're doing more and more of our product designs in virtual reality. We want to simulate our products and simulate its capabilities in simulation in this computer rather than build it in the beginning. Then the last category would be graphics virtualization. We've taken with GRID and our QUADRO virtual workstation, and now with NVIDIA RTX, we've turned the data center into a powerful graphics super computer. So, these are the various applications and segments of data centers that we see.\nI think in the case of training, we're limited by the number of deep learning experts in the world. That's growing very quickly. The frameworks are making it easier. There's a lot more open source and open documentation on sharing of knowledge and so the number of AI engineers around the world is growing super fast. The second is inference and I've already talked about that. It's really limited by our optimizing compilers and how we can target these neural network models to run our processors. If we can do so, we're going to save our customers enormous amounts of money. We speed up applications, we speed up these neural network models 50 times, 100 times, 200 times over a CPU. So, the more GPUs they buy, they're more they're going to save.\nHigh-performance computing, the way to think about that is I think at this point, it's very clear that going forward, super computers are going to get built with accelerators in them. Because of our long-term dedication to CUDA and our GPUs for acceleration of all these codes and the nurturing of the ecosystem, I think that we're going to do super well in the super computing world. These are different verticals.\nWith respect to competition, it all starts with the core. The core is that the CPU scaling has slowed. The world needs another approach going forward. Surely, because of our focus on it, we find ourselves in a great position. Google announced TPU 3 and it's still behind our Tensor Core GPU. Our Volta is our first generation of a newly reinvented approach of doing GPUs. It's called Tensor Core GPUs. We're far ahead of the competition. But more than that, it's programmable. It's not one function; it's programmable. Not only is it faster, it's also more flexible. As a result of the flexibility, developers can use it in all kinds of applications, whether it's medical imaging or whether simulations, or deep learning, or computer graphics.\nAs a result, our GPUs are available in every cloud and every data center everywhere on the planet, which developers need that accessibility so that they can develop their software. So, I think that on the one hand, it's too simplistic to compare a TPU to just one of the many features that's in our Tensor Core GPU, but even if you did, we're faster, we support more frameworks, we support all neural networks. As a result, if you look at GitHub, there's some 60,000 different neural network research papers that are posted that run on NVIDIA GPUs. It's just a handful for the second alternative. That just gives you a sense of the reach and capabilities of our GPUs.\nOperator\nYour next question comes from Toshiya Hari with Goldman Sachs.\nToshiya Hari -- Goldman Sachs -- Analyst\nGreat, thank you so much. Jensen, I had a question regarding your decision to pull the plug on your GeForce partner program. I think most of us read your blog from last Friday, I think it was, so we understand the basic background, but if you can describe what led to this decision and perhaps talk a little bit about the potential implications, if any, in terms of your ability to compete or gain share, that would be really helpful. Thank you so much.\nJensen Huang -- President and Chief Executive Officer\nYeah, thanks for the question, Toshiya. At the core, the program was about making sure that gamers who buy graphics cards know exactly the GPU brand that's inside. The reason for that is because we want gamers to -- the gaming experience of a graphics card depends so much on the GPU that is chosen. We felt that using one graphics card brand and interchanging the GPU underneath causes it to be more opaque and less transparent for the gamers to choose the GPU brand that they wanted. Most of the ecosystem loved it. Some of the people really disliked it. Instead of all that distraction, we're doing so well. We're going to continue to help the gamers choose the graphics cards, like we always have, and things will sort out. We decided to pull the plug because the distraction was unnecessary and we have too much good stuff to go do.\nOperator\nNext question is from C.J. Muse with Evercore ISI.\n\n J. Muse -- Evercore ISI -- Analyst\n\nHi, this is [inaudible] calling in for C.J. Muse. Thank you for taking my question. I have a question on HPC. [Inaudible] on the recent call raised their accelerator attach rate forecast on HPC to 50% from mid-teens. I would love to get further details on what exactly NVIDIA is doing to software services, etc. that's kind of creating this competitive positioning in HPC and AI, basically. Then if I could ask a follow-up, basically, on benchmarks. So, there's been some news on AI benchmarks, whether it's [inaudible], etc., so I would love to get your thoughts on (a) the current state of benchmarks for AI workloads; and (b) the relative positioning of [inaudible] versus GPUs, especially as we move toward newer networks like [inaudible], etc. Thank you.\nJensen Huang -- President and Chief Executive Officer\nThanks for the question. Well, HPC -- first of all, at the core, CPU scaling has stalled and it's reached the limits of physics. The world needs another approach to go forward. We created the GPU computing approach a decade and a half ago and I think at this point, with the number of developers jumping on, the number of applications that are emerging, it's really clear that the future of HPC has accelerated.\nOur GPU approach, because of its flexibility, because of its performance, because of the value that we create, as a result of the throughput of a data center, we save people so much money, just in cables alone. Oftentimes, it more than pays for the GPUs that they buy. The reason for that is because the number of servers reduce dramatically. So, I think the future of HPC is about acceleration and the NVIDIA CUDA GPUs are really in a great position to serve this vacuum that's been created.\nWith respect to benchmarks, you might have seen that earlier this week, we released three speed records: the fastest single GPU; the fastest single computer node -- a definition of a computer node is something that fits in a box that runs one operating system, one node; and one instance, one cloud instance. We now have the fastest speed record for one GPU, one node, and one instance. We love benchmarks. Nothing is more joyful than having a benchmark to demonstrate your leadership position. In the world of deep learning, the number of networks is just growing so fast because the number of different applications that deep learning is able to solve is really huge.\nAnd so, you need a lot of software capability and the versatility of your platform needs to be great. We also have a lot of expertise in the company in software. NVIDIA is really a full stack computing company. From architecture to system software, to algorithms, to applications, we have a great deal of expertise across the entire stack. And so, we love these complicated benchmarks that are out there and I think this is great way for us to simplify our leadership position.\nI think long-term, the number of networks that are going to emerge will continue to grow. The flexibility of ASICs is going to be its greatest downfall. If someone were to create a general purpose, parallel accelerating processor like ours and had it designed to be incredibly good at deep learning -- like recently what we did with our Tensor Core GPU, which is a reinvented GPU, and Volta is the first one -- it's going to be hard, it's going to be expensive, and we've been doing it a very long time. And so, I think it's a great time for us.\nOperator\nNext question is from Blayne Curtis with Barclays.\nBlayne Curtis -- Barclays -- Analyst\nThanks for my question. I wanted to ask on the inference side of [inaudible] and beyond autos when we look at sizing that TAM, what are the other big areas that you think you can penetrate with GPUs and [inaudible] besides autos?\nJensen Huang -- President and Chief Executive Officer\nBlayne, the largest inference opportunity for us is actually in the cloud in the data center. That's the first great opportunity. The reason for that is there's just an explosion in the number of different types of neural networks that are available. There is imagine recognition, there is video sequencing, there's recommender systems, there's speech recognition, speech synthesis, natural language understanding. There's just so many different types of neural networks that are being created.\nCreating one ASIC that can be adapted to all of these different types of networks is just a real challenge. By the time that you create such a thing, it's called a Tensor Core GPU, which is what we created. So, I think the first opportunity for us in large scale opportunity will be in the data center in the cloud.\nThe second will be in vertical markets. The vertical market you mentioned is self-driving cars. We see a great opportunity in autonomous vehicles, and I've mentioned that before. Between now and the time that we ramp our AV computers we call DRIVE, we're going to be selling a whole lot of servers so that companies could develop their neural network models for their self-driving cars, as well as simulating virtual reality. There are various test drives. As well as testing their neural network and their self-driving car stack against billions and billions of miles of saved up, pre-recorded videos. And so, in the vertical markets, we're going to see inference both in the data center for developing the self-driving car stack, as well as in the self-driving cars themselves.\nNow, in the self-driving cars, the ASPs for Level 2 could be a few hundred dollars to a Level 5 self-driving car, taxi, or driverless taxi being a few thousand dollars. I expect that driverless taxis will start going to market about 2019 and self-driving cars probably somewhere between 2020 and 2021. I think the size of the market is fairly well modeled. The simple way to think about that is I believe that everything that moves someday will be autonomous or have autonomous capabilities. So, the 100 million cars, the countless taxis, all the trucks, all the agriculture equipment, all the pizza delivery vehicles. You name it. Everything is going to be autonomous and the market opportunity is going to be quite large. That's the reason why we're so determined to go create that market.\nOperator\nYour next question is from Tim Arcuri with UBS.\nTim Arcuri -- UBS -- Analyst\nThank you. I actually wanted to go back to the question about seasonality for gaming in June. Normal seasonal sounds like it's up mid-teens for June in gaming, but obviously the comps are skewed a little bit because of the channel restock and the crypto stuff. Does the guidance for June assume that gaming is better or worse than that mid-teens normal seasonal? Thank you.\nJensen Huang -- President and Chief Executive Officer\nWe are expecting Q2 to be better than seasonality, if I understand your question. We're expecting Q2 to be better than Q1, and we're expecting Q2 to be better than seasonality. Did that answer your question?\nOperator\nYour next question is from Atif Malik with Citi.\nAtif Malik -- Citi -- Analyst\nHi, thanks for taking my question and good job on the results. I have a question for Colette. Colette, first, thank you for breaking out crypto sales in the OEM line and guide for us. I have a question on your gross margins. Your gross margins have been expanding on product mix, despite component pricing headwinds on the D-Ram side. When do you expect component pricing to become a tailwind for your gross margin?\nColette Kress -- Executive Vice President and Chief Financial Officer\nThanks so much for the question. When you think about our gross margins just over this last quarter, as you know, we were working on stabilizing the overall supply that was out there in the market for consumer GPUs. We benefited from that with a higher gross margin, as we filled and completed that. You've seen us absorb a significant amount of the component pricing changes that we have seen, particularly around the memory. We're not here to be able to forecast generally when those pricing for those components will stabilize. But we believe in terms of the value added that our platforms provide the components are an important part of finishing that, but I think we have a tremendous amount more value that we are adding in terms of the software on top of our platforms, which is enabling our gross margins.\nOperator\nYour next question is from Chris Caso with Raymond James.\nChris Caso -- Raymond James -- Analyst \nYes, hi, thanks for taking the question. The question is the progress on the deployment of Volta into the cloud service providers. You talked in your prepared remarks about 5 deployments, including the Google beta. Can you talk about how soon we can expect to see some of those remaining deployments? Of those already launched, how far are they along? What, I'd guess you'd say proverbially, what inning are we in in these deployments?\nJensen Huang -- President and Chief Executive Officer\nYeah, so first of all, Volta is a reinvented GPU. Volta is the world's first GPU that has been designed to be incredibly good at deep learning. We call it the Tensor Core GPU. It still retains all of the flexibilities of everything that CUDA's ever run is backwards compatible with everything that runs on CUDA, but it has new architectures designed to be incredibly good at deep learning. We call it a Tensor Core GPU. That's the reason why it has all of the benefits of our GPU, but none of the ASICs can catch up to it. And so, Volta is really a breakthrough. We're going to be very successful with Volta. Every cloud will have it.\nThe initial deployment is for internal consumption. Volta has been shipping to the cloud providers, the internet service companies, for the vast majority of the last quarter, as you guys know. They're using it internally. Now they're starting to open up Volta for external consumption to their cloud customers. They're moving as fast as they can. My expectation is that you're going to see a lot more coming online this quarter.\nOperator\nYour next question comes from Mark Lopasif with Jefferies.\nMark Lopasif -- Jefferies -- Analyst\nHi, thanks for taking my question. I had a question about the DGX family of products. Our own fieldwork is indicating very positive reception for DGX and I was wondering if you could help us understand the high growth you've seen in the data center versus to what extent is that being driven by the DGX and what, when DGX-2 starts to ramp in the back half of the year, is this something that kind of layers on top of DGX? Does DGX-2 layer on top of DGX? Are [inaudible] being segmented in the market or are these different products? Any color on how to think about those two products [inaudible] would be helpful. Thank you.\nJensen Huang -- President and Chief Executive Officer\nHey, Colette, could you give me a brief version of that? It was kind of crackling on my side.\nColette Kress -- Executive Vice President and Chief Financial Officer\nI'm going to ask the operator if they could ask for the question again because it was also on our side a little crackly.\nOperator\nYes. Mark, your line is open. Please restate your question.\nMark Lopasif -- Jefferies -- Analyst\nOkay, thanks. Can you hear me better now?\nJensen Huang -- President and Chief Executive Officer\nYeah, much better Mark.\nMark Lopasif -- Jefferies -- Analyst\nOkay, sorry about that. I'm at the airport. The question was on the DGX family of products. Our own fieldwork indicates a very positive reception. I was wondering, Jensen, if you could help us understand the high growth you've seen in the data center market, how much is DGX contributing to that? And then when DGX-2 starts to ramp in the second half of the year, how do we think about DGX 1? Does it replace the original DGX or are you going after different segments? Do they layer on top of one another? Any color on that would be helpful. Thank you.\nJensen Huang -- President and Chief Executive Officer\nI see. Thank you. DGX-2 and DGX-1 will both be in the market at the same time. DGX is a few hundred million dollar business. It was introduced last year, so its growth rate is obviously very high. It's designed for enterprises where they need to have their computers on premise, but they don't want to build a super computer and they don't have the expertise to do so. They would like to pull a super computer out of a box, plug it in, and start doing super computing.\nAnd so, DGX is really designed for enterprises. It's designed for car companies, it's designed for healthcare companies doing life sciences work or medical imaging work. We recently announced a project called Project Clara, which basically takes medical imaging equipment, virtualizes them, containerizes the software, and turns it into a -- most medical imaging equipment today are computational and a lot of them run on NVIDIA CUDA anyways. And so, we can put that into the data center, we can virtualize their medical instruments, and it gives them the opportunity to upgrade the millions of instruments that are out in the marketplace today.\nAnd so, DGX is really designed for enterprises. We're seeing great success there. It's really super easy to use and it comes with direct support from HPC and AI researchers at NVIDIA and the answer to your question at the end is, both of them will be in the market at the same time.\nOperator\nThe next question is from Mitch Steves with RBC Capital Markets.\nMitch Steves -- RBC Capital Markets -- Analyst\nHey, guys. I'm actually going to go with a more nitty-gritty question just on the financial side, just to make sure I'm understanding this right. So, the OEM beat was pretty material, given a lot of crypto revenue. Is it still the case that OEM is materially lower gross margin than your corporate average at this time?\nColette Kress -- Executive Vice President and Chief Financial Officer\nSure, I'll take that question. Generally, our OEM business can be a little bit of volatile because, remember, OEM business incorporates our mainstream GPUs, as well as our Tegra Integrated. So, we have development platforms that we sell on some of the Tegra piece of it, but they are slightly below and I think you can go back and refer to our discussion at Investor Day, as there was a slide there that talks about those embedded pieces, and then being below.\nSo, yes, you're correct. Again, a very small part of our business right now.\nOperator\nYour next question comes from Christopher Rolland with Susquehanna.\nChris Rolland -- Susquehanna -- Analyst\nHey, guys. Thanks for the question. So, your competitor thinks that just 10% of their sales were from crypto, or like $150, $160 million. You guys did almost $300 million there. Perhaps, I think, there could actually be some in gaming as well, which would imply that you guys have two-thirds or more of that market? So, I guess, what's going on there? Is there a pricing dynamic that's allowing you to have such share there or do you think it's your competitors that don't know what's actually being sold to miners versus gamers? Why such implied share in that market? Thanks.\nJensen Huang -- Founder, President and Chief Executive Officer\nWell, we try to as transparently reveal our numbers as we can. Our strategy is to create a skew that allows the crypto miners to fulfill their needs and we call it CMP. As much as possible, fill their demand that way. Sometimes it's just not possible because the demand is too great. But we try to do so. We try to keep the miners on the CMP skews as much as we can. I'm not exactly sure how other people do it, but that's the warranty we do it.\nOperator\nYour next question is from Craig Ellis with B. Riley.\nCraig Ellis -- B. Riley -- Analyst\nThanks for sneaking me in and congratulations on all the financial records in the quarter. Jensen, I just wanted to come back to an announcement that you made at GTC with ray tracing. Because the technology looked like it was very high fidelity and I think you noted at the time it was very computationally intensive. So the question is, as we think about the gaming business and the potential for ray tracing to enter that platform group, what does it mean for dynamics that we've seen in the past, for example, the ability to really push the high end of the market with high-end capability 1070 TI had launched last year; it was very successful. Does this give you further flexibility for those types of launches as you bring exciting and very high-end technology to market? Thank you.\nJensen Huang -- Founder, President and Chief Executive Officer\nYeah, I appreciate it. NVIDIA RTX is the biggest computer graphics invention in the last 15 years. It took us a decade to do. We literally worked on it continuously for one decade. To put it in perspective, it's basically film rendering, cinematic rendering, except it's in real time. It merges the style of computer graphics, restorization, and life simulation, what people call ray tracing, as well as deep learning and AI, and merged it into one, unified framework so that we can achieve cinematic rendering in real time. What is currently takes is a server about a few hours, depending on the scene, it might take as long as a full day, take a few hours to render one frame.\nSo it takes a server, one node of a server, several hours to render one frame. In order to render 30 frames per second, just imagine the number of servers you need. If it take several hours per frame and you need to render 30 frames per second in order to be real time, it basically takes a high performance computer or a super computer a render farm -- that's why they call it a render farm -- it's a full data center designed just for rendering.\nNow, we've created NVIDIA RTX, which makes it possible to do in real time. We demonstrated RTX on four QUADRO GV100s. It takes four of our latest generation Volta Tensor Core GPUs to be able to render 30 frames per second the Star Wars cinematic that people enjoy. The amount that we saved, we basically took an entire data center and reduced it into one node. We're now doing it in real time. So, the amount of money that we can save people who create movies, people who do commercials, people who use film rendering to create the game content -- almost every single game is done that way -- there's quite a bit of offline rendering to create the imagery and the textures and the lighting.\nThen, of course, architecture design and car design. The number of applications, the number of industries that are built on top of modern computer graphics is really quite large. I'm certain that NVIDIA RTX is going to impact every single one of them. So, that's our starting point is to dramatically reduce the cost of film rendering, dramatically reduce the time that it takes to do it, and hopefully more GPU servers will be purchased and, of course, better content will be created. Long-term, we've also now plotted the path toward doing it in real time. Someday, we will be able to put RTX into a GeForce gaming card and the transformation to the revolution to the gaming industry will be quite extraordinary. We're super excited about RTX.\nOperator\nYour next question is from Stacy Rasgon with Bernstein.\nStacy Rasgon -- Bernstein -- Analyst\nHi, guys. Thanks for fitting me in for my follow-up. This is a question for Colette. I want to follow up again on the seasonality. I'm understanding the prior comments. Normal seasonal for Q2 for gaming would be up in the double digits. Given your commentary on the crypto decline into Q2, given your commentary on just the general drivers around data center and the Volta ramps, I can't bring that together with the idea of gaming being above seasonal within the context of your guidance envelope. How should I reconcile those things? How are you actually thinking about seasonality for gaming into Q2 within the context of the scenarios that are currently contemplated in your guidance for next quarter?\nColette Kress -- Executive Vice President and Chief Financial Officer\nSure, Stacy. Let me see if I can bridge together Jensen and then some comments here. Unfortunately, they're moving quite fast to the next question, so I wasn't able to add on. But let me see if I can add on here and present a little bit of clarity in terms of the seasonality. Remember, in Q1, we outgrew seasonality significantly. We left Q4 with very low inventory in terms of the new channel. We spent Q1 working on establishing a decent amount of inventory available. We wanted to concentrate on our miners separately. You can see we did that in terms of Q1 by moving that to OEM and moving that to cryptocurrency only boards.\nSo, we left Q1 at this point with healthy overall channel inventory levels as far as where we stand. That then takes you now to Q2. But, if we overshot in terms of seasonality in terms of Q1, we don't have to do those channel fill dynamics again as we get into Q2. But we do have demand out there for our gamers that we can now address very carefully with the overall inventory that we now have available. So, putting together Q1 and Q2 together, yes, we are within normal seasonality. Again, for a guidance, and we'll see how we'll finish in terms of the quarter, but you should be in that range. So, yes, from a normal seasonality at a year-to-date inclusive of Q2? Yes, we're on that overall seasonality.\nAlways keep in mind generally our H2s are usually higher than our overall H1s. That's what you should think about our overall guidance. Gaming is still strong. We have to comment that our overall drivers that have taken us to this place over the last 3 to 5 years with phenomenal growth and our ability to grow that overall market is still here and all of those things are together. We've just had a few quarters in terms of making sure that we get the overall channel correct and put our miners separately. I hope that clarifies in terms of where we are in terms of gaming seasonality.\nOperator\nYour last question comes from Will Stein with SunTrust.\nWilliam Stein -- SunTrust -- Analyst\nGreat, thank you for taking my question and squeezing me in. The question relates to the supply chain challenges that you've talked so much about in the gaming end market. I'm wondering if there's something particular to that end market that is making the shortages concentrated there or are, in fact, other end markets, in particular the data center end market also somewhat restricted from what growth they might have achieved if there weren't the shortages that are out there, and maybe talk about the pace of recovery of those, that'd be really helpful. Thanks so much.\nColette Kress -- Executive Vice President and Chief Financial Officer\nLet me start off here and I'll have Jensen finish up on the last part of that question. But overall, our data center business did phenomenal. Volta is doing extremely well and even now with 32-bit, we're seeing tremendous adoption throughout. Again, remember it's very different than the overall consumer business. We have significant amount of time for qualification and that is moving extremely fast based on a lot of other industries and their ability to qualify. So, no, there is not a supply challenge at all in terms of the data center and our overall growth in data center we're extremely pleased with in terms of how the quarter came out. I'll turn it over to you, Jensen, and you can answer the rest of the part of it?\nJensen Huang -- Founder, President and Chief Executive Officer\nYeah, the reason why miners love GeForce is because miners are everywhere in the world. One of the benefits of cryptocurrency is that it's not any sovereign currency. It's in the digital world. It's distributed. GeForce is the single, largest distributed supercomputing infrastructure on the planet. Every gamer has a super computer in their PC. GeForce is so broadly distributed, it's available everywhere. And so, GeForce is really a good candidate for any new cryptocurrency or any new cryptography algorithm that comes along.\nWe try the best we can to go directly to the major miners. They represent the vast majority of the demand. To the best of our ability, we serve their needs directly and we call that CMP and that's why it's not called GeForce, they're called CMP. We can serve those miners directly, hopefully to take some of the demand pressure off of the GeForce market because ultimately, what we would like is we would like the market for GeForce pricing to come down so that the gamers could benefit from the GeForces that we built for them and the gaming demand is strong. The bottom line is Fortnite is a home run. The bottom line is PUBG is a home run. The number of gamers that are enjoying these games is really astronomic, as people know very well.\nIt's a global phenomenon. These two games are equally fun in Asia as it is in Europe, as it is in the United States. Because you team up and this is a battle royale, you'd rather play with your friends, so it's incredibly social. It's incredibly sticky. More gamers that play, more of their friends join and more of their friends join, more gamers that play. So it's this positive feedback system. The guys at Epic did a fantastic job of creating Fortnite. It's just a wonderful game genre that people are really enjoying.\nI think at the core of it, the gaming is strong and we're looking forward to inventory normalizing in the channel so that pricing could normalize in the channel, so that gamers can come back to buy the GeForce card that is now been in short supply for over a quarter. And so the pent-up demand is quite significant and I'm expecting the gamers to be able to buy new GeForces pretty soon.\nOperator\nUnfortunately, we have run out of time. I will now turn it back over to Jensen for any closing remarks.\nJensen Huang -- Founder, President and Chief Executive Officer\nLet's see here. Is it my turn again?\nColette Kress -- Executive Vice President and Chief Financial Officer\nIt is.\nJensen Huang -- Founder, President and Chief Executive Officer\nOkay, we had another great quarter. Record revenue. Record margins. Record earnings. Growth across every platform. Data center achieved another record with strong demand for Volta and AI inference. Gaming was strong. We're delighted to see prices normalizing and we can better serve pent-up gamer demand. At the heart of our opportunity is the incredible growth of computing demand of AI. Just as traditional computing has slowed, the GPU computing approach that we've pioneered is ideal for filling this vacuum. Our invention of the Tensor Core GPU has further enhanced our strong position to power the AI era. I look forward to giving you another update next quarter. Thank you.\nOperator\nThis concludes today's conference call. Thank you for joining. You may now disconnect.\nDuration: 66 minutes\nCall participants:\nJensen Huang -- President and Chief Executive Officer\nColette Kress -- Executive Vice President and Chief Financial Officer\nSimona Jankowski -- Vice President, Investor Relations\nStacy Rasgon -- Bernstein Research -- Analyst\nJoe Moore-Morgan Stanley -- Analyst\nVivek Arya -- Bank of America -- Analyst\nToshiya Hari -- Goldman Sachs -- Analyst\n\n J. Muse -- Evercore ISI -- Analyst\n\nBlayne Curtis -- Barclays -- Analyst\nTim Arcuri -- UBS -- Analyst\nAtif Malik -- Citi -- Analyst\nChris Caso -- Raymond James -- Analyst \nMark Lopasif -- Jefferies -- Analyst\nMitch Steves -- RBC Capital Markets -- Analyst\nChris Rolland -- Susquehanna -- Analyst\nCraig Ellis -- B. Riley -- Analyst\nWilliam Stein -- SunTrust -- Analyst\nMore NVDA analysis\nThis article is a transcript of this conference call produced for The Motley Fool. While we strive for our Foolish Best, there may be errors, omissions, or inaccuracies in this transcript. As with all our articles, The Motley Fool does not assume any responsibility for your use of this content, and we strongly encourage you to do your own research, including listening to the call yourself and reading the company's SEC filings. Please see our Terms and Conditions for additional details, including our Obligatory Capitalized Disclaimers of Liability.\n10 stocks we like better than NvidiaWhen investing geniuses David and Tom Gardner have a stock tip, it can pay to listen. After all, the newsletter they have run for over a decade, Motley Fool Stock Advisor, has quadrupled the market.*\nDavid and Tom just revealed what they believe are the 10 best stocks for investors to buy right now... and Nvidia wasn't one of them! That's right -- they think these 10 stocks are even better buys.\nClick here to learn about these picks!\n*Stock Advisor returns as of May 8, 2018\n                                            \n                                        "}]